\section{Related Work}
AutoLibra unifies three areas of research: it draws inspiration from \textit{thematic analysis} to create \textit{nautral language-derived evaluation metrics} to evaluate and reward \textit{AI agents}. 

\paragraph{Evaluating AI agents} Much of the work in AI agent evaluation focuses around benchmarks which contains both task suites and evaluation metrics. In addition to the datasets we used in this paper, SWE-Bench \citep{jimenezswe} uses human-written unit tests as evaluation metrics; Embodied Agent Interface \citep{li2024embodied} provides fine-grained evaluation for LLM-based embodied agents; $\tau$-Bench \citep{yao2024tau} compares database states for evaluation; concurrent work AgentRewardBench \citep{l√π2025agentrewardbenchevaluatingautomaticevaluations} builds a benchmark for reward models for web agents. Recently, there are observatory tools including Galileo \citep{galileo_agentic}, Vertex AI Gen AI \citep{google_agent_eval}, and Docent \citep{meng2025docent} which provide user interfaces to visualize agent failure modes. Generating intrinsic rewards have also been studied in the reinforcement learning community \citep{du2019liir,pathakICMl17curiosity,laskin2022cic} to encourage exploration, sub-task completion, or skill discovery. In contrast to these, AutoLibra is a pure data-driven task-agnostic method without predefined failure taxonomy for generating interpretable metrics for agents. 

\paragraph{Learning from natural language and human feedback} Researchers have been studying reinforcement learning with language feedback to provide a dense reward to agents \citep{goyal2019using}. Since LLM agents are even harder to train with sparse reward, there is substantial interest in training LLM agents from natural language feedback. \citet{chen2024learning} propose an imitation learning method for learning from human feedback; Text2Reward \citep{xietext2reward} uses code generation to generate robot reward functions from open-ended human feedback; our work \citep{chen2025fine} uses feedback to the improvement agent policy with prompting and then align the unprompted agent policy with the prompted one; \citet{shi2024yell} propose a new model architecture to incorporate human feedback into policy learning. On the other hand, human non-open-ended feedback is also incorporated in training agents, including rating feedback \citep{nguyen2017reinforcement}, preference feedback \citep{christiano2017deep}, demonstrative feedback \citep{shaikhaligning}.
Unlike these papers, AutoLibra induces metrics from feedback from all annotated instances and generates metrics that are generalizable to different tasks and useful for both evaluation and agent fine-tuning.

\paragraph{Automatic thematic analysis} Thematic analysis is a powerful tool for qualitative study through coding and iterative creation of themes. \citet{gauthier2022computational} provide computational tools to aid this process; \citet{hong2022scholastic} and \citet{gebreegziabher2023patat} explore human-AI collaboration in thematic analysis; LLooM \citep{lam2024concept}, is an automatic method for concept induction, is closest to and influences our approach. This paper completes the loop of concept induction by using the meta-evaluation step to optimize the induced metrics, and apply this social science technique to agent evaluation. 