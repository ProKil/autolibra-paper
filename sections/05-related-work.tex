\section{Related Work}
AutoLibra unifies three areas of research: it draws inspiration from \textit{thematic analysis} to create \textit{nautral language-derived evaluation metrics} to evaluate \textit{AI agents}. 

\paragraph{Evaluating AI agents} Much of the work in AI agent evaluation builds a benchmark which contains both task suites and evaluation metrics. In addition to the data sets we used in this paper, SWE-Bench \citep{jimenezswe} uses human-written unit tests as evaluation metrics; Embodied Agent Interface \citep{li2024embodied} provides fine-grained evaluation for LLM-based embodied agents; $\tau$-Bench \citep{yao2024tau} compares database states for evaluation. In contrast to these, AutoLibra is a task-agnostic method for generating metrics. 

\paragraph{Learning from natural language feedback} Recently, there is substantial interest in training agents from natural language feedback. \citet{chen2024learning} propose an imitation learning method for learning from human feedback; Text2Reward \citep{xietext2reward} uses code generation to generate robot reward functions from open-ended human feedback; \citet{shi2024yell} propose a new model architecture to incorporate human feedback into policy learning. Unlike these papers, AutoLibra induces metrics from all annotated instances and generates metrics that are useful for both evaluation and agent fine-tuning.
\paragraph{Automatic thematic analysis} Thematic analysis is a powerful tool for qualitative study through coding and iterative creating themes. \citet{gauthier2022computational} provide computational tools to aid this process; \citet{hong2022scholastic} and \citet{gebreegziabher2023patat} explore human-AI collaboration in thematic analysis; LLooM \citep{lam2024concept}, is an automatic method for concept induction, is closest to and influences our approach. This paper completes the loop of concept induction by using the meta-evaluation step to optimize the induced metrics, and apply this social science technique to agent evaluation. 