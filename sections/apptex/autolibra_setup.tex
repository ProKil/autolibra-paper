This section describes the test configuration used during AutoLibra Ladder
experiments, as described in \ref{sec:ladder}.

For each environment experiment, one unmodified agent is used as a baseline for
comparison (\textit{Iteration 0}), and three complete iterations of iterative
agent improvement with AutoLibra are performed (\textit{Iterations 1-3}), for a
total of four iterations. Six representative tasks for baba-is-ai are used in to
induce metrics within the AutoLibra pipeline, with the remaining 34 tasks held
out for evaluation. The AutoLibra Ladder pipeline is evaluated on the baba-is-ai
and MiniHack environments; any changes to the agent code, the environment score,
trajectory performance, and other metrics are recorded at the end of each iteration.
GPT-4o-241120 \cite{openai2024gpt4ocard} is used as the agent model in all
experiments; the human-in-the-loop configuration of the AutoLibra pipeline is used.