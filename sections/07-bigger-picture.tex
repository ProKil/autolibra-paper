\documentclass[../main.tex]{subfiles}

\begin{document}
	\section{The Bigger Picture}

	Bring human in the loop of the developing machine learning models has been 
    studied for decades \citep{monarch2021human}. The raise of AI systems with
    agency bring new opportunies and challenges of this mission, 

	\textbf{Human-in-the-Loop Evaluation.} AutoLibra transforms casual human
	observations into structured evaluation frameworks. Rather than requiring domain
	experts to anticipate all possible evaluation criteria upfront, our approach
	allows end-users to provide natural language feedback that gets systematically
	converted into actionable metrics. This democratizes agent evaluation,
	enabling non-technical stakeholders to contribute meaningfully to the
	assessment process.

	\textbf{Iterative Agent Training.} By converting human feedback into concrete metrics,
	AutoLibra enables a new paradigm of iterative agent improvement. Developers can
	collect feedback from diverse users, automatically generate evaluation
	criteria, and use these metrics to guide both prompt engineering and model training.
	This creates a feedback loop where human insights directly inform agent
	optimization, moving beyond simple task success metrics to capture nuanced behavioral
	preferences.

	\textbf{Scalable Human Oversight.} Perhaps most importantly, AutoLibra provides
	a pathway for scalable human oversight of autonomous systems. As agents become
	more capable and deployed in diverse contexts, the ability to quickly translate
	human concerns into systematic evaluation becomes crucial. Our framework
	enables organizations to maintain human agency in agent development while
	scaling beyond what manual evaluation could achieve.

	This work lays the groundwork for a future where agent development is
	inherently collaborative between humans and machines, ensuring that autonomous
	systems remain aligned with human values and expectations throughout their evolution.
\end{document}