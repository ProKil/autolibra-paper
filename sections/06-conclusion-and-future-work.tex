\section{Conclusion and Future Work}
In this paper, we propose AutoLibra, an evaluation paradigm and induction method for agent evaluation metrics based on open-ended human feedback.
We find that this framework is generalizable to a diverse range of agent tasks, provides new insights into agent behaviors,
and identifies strong optimization targets for agent improvements via either prompt engineering or finetuning.
Future work could apply the same framework on other agent tasks and explore the use of the induced metrics as reward signals for reinforcement learning.