\documentclass{article} % For LaTeX2e
\usepackage[preprint]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}

\usepackage{lineno}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{tikz}
\usetikzlibrary{positioning, fit, arrows.meta, shapes.geometric}
\usepackage{wrapfig}
\usepackage{colortbl} % For row coloring
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{fancyvrb}
\usepackage{etoolbox}

% Algorithm formatting
\usepackage{algorithm}
\usepackage{algpseudocode}
\newcommand{\diyi}[1]{{\color{red} Diyi: #1}}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{AutoLibra \protect\includegraphics[height=1em]{figs/scale.png} \\ \textit{Metric Induction for Agents from Open-Ended Human Feedback}}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Hao Zhu\raisebox{0.5em}{\includegraphics[height=1em]{figs/robot.png}} Phil Cuvin\raisebox{0.5em}{\includegraphics[height=1em]{figs/microscope.png}} Xinkai Yu\raisebox{0.5em}{\includegraphics[height=1em]{figs/ladder.png}} Charlotte Ka Yee Yan\raisebox{0.5em}{\includegraphics[height=1em]{figs/robot.png}} Jason Zhang\raisebox{0.5em}{\includegraphics[height=1em]{figs/robot.png}} Diyi Yang\raisebox{0.5em}{\includegraphics[height=1em]{figs/robot.png}}\\
\raisebox{0.5em}{\includegraphics[height=1em]{figs/robot.png}}Stanford University \raisebox{0.5em}{\includegraphics[height=1em]{figs/microscope.png}}University of Toronto \raisebox{0.5em}{\includegraphics[height=1em]{figs/ladder.png}}University of Pennsylvania\\
\texttt{\{zhuhao,ckyy,jasonbz,diyiy\}@stanford.edu}\\
\texttt{philippe.cuvin@mail.utoronto.ca},
\texttt{xinkaiyu@sas.upenn.edu}\\\\
\href{https://github.com/Open-Social-World/autolibra}{Code}\quad\href{https://huggingface.co/datasets/open-social-world/autolibra}{Data}\quad Website: \url{https://autolibra.opensocial.world} 
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle
\vspace{-20pt}
\begin{abstract}
Agentic systems are predominantly evaluated and optimized via environment-specific success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. 

% approaches are either not fine-grained enough or require  manual design of the metrics by experts.
We thus propose \emph{AutoLibra},
a framework for agent evaluation and fine-tuning, that transforms open-ended human feedback 
\emph{e.g.} ``\textsf{If you find that the button is disabled, don't click it again}'',
or ``\textsf{This agent has too much autonomy to decide what to do on its own}''
into behavior-salient metrics for evaluating fine-grained task performance in agent trajectories.
AutoLibra accomplishes this by grounding feedback to an agent's behavior,
clustering similar positive and negative behaviors,
and creating behavior-specific metrics to prompt LLM-as-a-Judge with concrete examples and definitions. 
We further propose two \emph{meta-metrics} to evaluate the alignment of a set of (induced) metrics
with open feedback: ``coverage'' and ``redundancy''. Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to recover \textbf{agent evaluation} dimensions heuristically proposed in previous agent evaluation benchmarks and discover metrics reflecting newly emergent behavior. We additionally showcase two practical applications of AutoLibra in \textbf{agent improvement}:
First, we demonstrate that metrics induced by AutoLibra serve as better fine-tuning targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20\%.
% , enhancing prompt engineering. 
Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. 
Our results suggest that AutoLibra is a powerful, task-agnostic tool for evaluating and improving language agents.
\end{abstract}


\input{sections/01-introduction}


\input{sections/02-method.tex}

\input{sections/03-lens.tex}



\input{sections/04-ldr}
\input{sections/05-related-work}
\input{sections/06-conclusion-and-future-work}


\section*{Limitation and Ethics Statement}
The scope of this paper is text-based agents, which does not include agents with multimodal observation or action spaces. Within human-aided experiments, we are also limited by the diversity of human annotators. The annotation of the data in this paper, except for the feedback collected by \citet{shao2024collaborative}, is done by the authors, who are experts in AI agents. We do not address the effect of the experience of the annotators on the induced metrics. Finally, induced metrics should be used with caution, these could reflect the internal biases of the LLMs used to extract them. 



\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\appendix
\section{Appendix}
\input{sections/appendix}

\end{document}
